input {
    jdbc {
        jdbc_driver_class => "org.postgresql.Driver"
        jdbc_driver_library => "/usr/share/logstash/drivers/postgresql-42.2.5.jar"
        jdbc_connection_string => "jdbc:postgresql://db:5432/asts"
        #jdbc_connection_string => "jdbc:postgresql://db:5432/${POSTGRES_DB}"
        jdbc_validate_connection => true
        jdbc_user => "postgres"
        #jdbc_user => "${POSTGRES_USER}"
        jdbc_password => "root"
        #jdbc_password => "${POSTGRES_PASSWORD}"
        #tracking_column => "unix_ts_in_secs"
        #use_column_value => true
        #tracking_column_type => "numeric"
        schedule => "*/5 * * * * *"             #5초마다 실행
        #statement => "SELECT *, extract(epoch from last_modified_date) AS unix_ts_in_secs FROM movie WHERE (extract(epoch from last_modified_date) > :sql_last_value AND last_modified_date < NOW()) ORDER BY last_modified_date ASC"
        #schedule => "* * * * *"
        statement => "SELECT * from news where (crawled_date at time zone 'utc') > NOW() - (10 * interval '1 second') order by url ASC;"  #트랜잭션 시간때문에 5초에 한번씩 쿼리를 날리면 아무것도 안날라온다... (데이터가 많아지면 읽는 속도 때문에 더 늘려야할지도?)
        use_column_value => true
        tracking_column => "crawled_date"
        tracking_column_type => "timestamp"
    }
}
filter {
  date {
    locale => "ko"
    match => ["crawled_date","yyyy-MM-dd HH:mm:ss"]
    timezone => "Asia/Seoul"
    target => "crawled_date"
  }
  mutate {
    copy => { "id" => "[@metadata][_id]"}
    remove_field => ["id", "@version", "unix_ts_in_secs","@timestamp"]
  }
}

output {
    elasticsearch {
        hosts => ["elasticsearch:9200"]
        index => "news"
        document_id => "%{[@metadata][_id]}"            #document_id 를 통해서 같은 값이 들어왔을때 데이터를 덮어쓴다.
    }
    stdout{
        codec => rubydebug { }      # rubydebug는 출력을 보기좋은 json 포멧으로 보여준다.
    }
}